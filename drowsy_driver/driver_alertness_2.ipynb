{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "from collections import OrderedDict\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FileVideoStream\n",
    "from scipy.spatial import distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the shape predictor and alarm audio file\n",
    "SHAPE_PREDICTOR_PATH = \"/home/rey/shape_predictor_68_face_landmarks.dat\"\n",
    "#ALARM_AUDIO_PATH = './alarm.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ratio of eye landmark distances to determine if a person is blinking\n",
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    vertical_A = dist.euclidean(eye[1], eye[5])\n",
    "    vertical_B = dist.euclidean(eye[2], eye[4])\n",
    "    \n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    horizontal_C = dist.euclidean(eye[0], eye[3])\n",
    "    \n",
    "    # compute the eye aspect ratio\n",
    "    ear = (vertical_A + vertical_B) / (2.0 * horizontal_C)\n",
    "    \n",
    "    # return the eye aspect ratio\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant for the eye aspect ratio to indicate drowsiness \n",
    "EYE_AR_THRESH = 0.21\n",
    "\n",
    "# Constant for the number of consecutive frames the eye (closed) must be below the threshold\n",
    "EYE_AR_CONSEC_FRAMES = 48\n",
    "\n",
    "# Initialize the frame counter\n",
    "FRAME_COUNTER = 0\n",
    "\n",
    "# Boolean to indicate if the alarm is going off\n",
    "IS_ALARM_ON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a bounding predicted by dlib and convert it\n",
    "# to the format (x, y, w, h) as normally handled in OpenCV\n",
    "def rect_to_bb(rect):\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "    \n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dlib face landmark detector will return a shape object \n",
    "# containing the 68 (x, y)-coordinates of the facial landmark regions.\n",
    "# This fucntion converts the above object to a NumPy array.\n",
    "def shape_to_np(shape, dtype = 'int'):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype = dtype)\n",
    "    \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        \n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that maps the indexes of the facial\n",
    "# landmarks to specific face regions\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "    (\"mouth\", (48, 68)),\n",
    "    (\"right_eyebrow\", (17, 22)),\n",
    "    (\"left_eyebrow\", (22, 27)),\n",
    "    (\"right_eye\", (36, 42)),\n",
    "    (\"left_eye\", (42, 48)),\n",
    "    (\"nose\", (27, 35)),\n",
    "    (\"jaw\", (0, 17))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dlib's face detector (HOG-based)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# create the facial landmark predictor\n",
    "predictor = dlib.shape_predictor(SHAPE_PREDICTOR_PATH)\n",
    "\n",
    "# grab the indexes of the facial landmarks for the left and\n",
    "# right eye, respectively\n",
    "(leStart, leEnd) = FACIAL_LANDMARKS_IDXS['left_eye']\n",
    "(reStart, reEnd) = FACIAL_LANDMARKS_IDXS['right_eye']\n",
    "\n",
    "# Streaming from a web-cam\n",
    "vs = VideoStream(src = 0).start()\n",
    "fileStream = False\n",
    "\n",
    "time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-802e10255a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# it may allow us to detect more faces in the image — the downside is that the larger the input image,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# the more computaitonally expensive the detection process is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# loop over the detected faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over frames from the video stream\n",
    "while True:\n",
    "    # if this is a file video stream, then we need to check if\n",
    "    # there any more frames left in the buffer to process\n",
    "    ##if fileStream and not vs.more():\n",
    "    ##    break\n",
    "        \n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale channels\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width = 450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the grayscale image\n",
    "    # Note: The 0 in the second argument indicates that we will not upsample the image. \n",
    "    # The benefit of increasing the resolution of the input image prior to face detection is that \n",
    "    # it may allow us to detect more faces in the image — the downside is that the larger the input image, \n",
    "    # the more computaitonally expensive the detection process is.\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # loop over the detected faces\n",
    "    for rect in rects:\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape_np = shape_to_np(shape)\n",
    "        \n",
    "        # extract the left and right eye coordinates, then use the\n",
    "        # coordinates to compute the eye aspect ratio for both eyes\n",
    "        leftEye = shape_np[leStart:leEnd]\n",
    "        rightEye = shape_np[reStart:reEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "        \n",
    "        # average the eye aspect ratio together for both eyes\n",
    "        avgEAR = (leftEAR + rightEAR) / 2.0\n",
    "        \n",
    "        # compute the convex hull for the left and right eye, then\n",
    "        # visualize each of the eyes\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "        \n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        if avgEAR < EYE_AR_THRESH:\n",
    "            FRAME_COUNTER += 1\n",
    "            \n",
    "            # if the eyes were closed for a sufficient number of\n",
    "            # then sound the alarm\n",
    "            if FRAME_COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                # if the alarm is not on, turn it on\n",
    "                #if not IS_ALARM_ON:\n",
    "                 #   IS_ALARM_ON = True\n",
    "                    \n",
    "                    # check to see if an alarm file was supplied,\n",
    "                    # and if so, start a thread to have the alarm\n",
    "                    # sound played in the background\n",
    "                  #  if ALARM_AUDIO_PATH != '':\n",
    "                   #     t = Thread(target = sound_alarm, args = (ALARM_AUDIO_PATH,))\n",
    "                    #    t.deamon = True\n",
    "                     #   t.start()\n",
    "                        \n",
    "                cv2.putText(frame, 'DROWSINESS ALERT!!!', (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        else:\n",
    "            FRAME_COUNTER = 0\n",
    "            IS_ALARM_ON = False\n",
    "            \n",
    "        # draw the computed eye aspect ratio for the frame\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(avgEAR), (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "    # show the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
